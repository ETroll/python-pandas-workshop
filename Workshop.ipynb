{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Pandas (and a bit of numPy) introduction Workshop\n",
    "\n",
    "DISCLAIMER: I am normally a C# developer and my style of coding might not always be following the \"pythonic\" way. I am no fan of snake case and prefer camelCase for functions and variables and PascalCase for Classes. Even if this might shine trough in this document, you can write the code however you like. If you want to learn Python the \"pythonic\" way, then use snake case for variables and functions.\n",
    "\n",
    "This workbook is created for Python 3.6.x and above. Some of the language basics shown here is not available for earlier versions of Python.\n",
    "\n",
    "Examples provided in this notebook should suffice to solve most of the exercises given. But if you want to get right to the documentation for the libraries we are using then you can quickly access them using the links below:\n",
    "\n",
    " - Python documentation: [docs.python.org](docs.python.org).\n",
    " - Jupyter documentation: [jupyter-notebook.readthedocs.io](https://jupyter-notebook.readthedocs.io/en/stable/notebook.html).\n",
    " - NumPy documentation: [docs.scipy.org](https://docs.scipy.org/doc/numpy/user/index.html)\n",
    " - Pandas documentation [pandas.pydata.org](https://pandas.pydata.org/pandas-docs/stable/)\n",
    " \n",
    "\n",
    "## Workbook organization\n",
    "\n",
    "The workbook in organized with explanation, examples and exercies all in same workbook. The exercies will come at multiple points during the workbook and there is no single placement of all exercises like last. So best way is to read the workbok top to bottom and do the exercises along the way.\n",
    "\n",
    "If you want direct access to the exercieses the following links should take you there:\n",
    "\n",
    " - [Exercise 1 - Testing out Jupyter](#Exercise-1---Testing-out-Jupyter)\n",
    " - [Exercise 2 - Multiple cells](#Exercise-2---Multiple-cells)\n",
    " - [Exercise 3 - Wash data (NumPy)](#Exercise-3---Wash-data-(NumPy))\n",
    " - [Exercise 4 - More washing of data (NumPy)](#Exercise-4---More-washing-of-data-(NumPy))\n",
    " - [Exercise 5 - Import data (Pandas)](#Exercise-5---Import-data)\n",
    " - [Exercise 6 - Wash the data (Pandas)](#Exercise-6---Wash-the-data)\n",
    " - [Exercise 7 - Filtering (Pandas)](#Exercise-7---Filtering)\n",
    " - [Exercise 8 - Create a DataFrame using results (Pandas)](#Exercise-8---Create-a-DataFrame-using-results)\n",
    " - [Exercise 9 - Create new columns / features (Pandas)](#Exercise-9---Create-new-columns-(features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Jupyter Notebooks crash course\n",
    "\n",
    "This is a Jupyter notebook. Opening this notebook is a proof that you have read the initial `README` file for this workshop. Congratulations!\n",
    "\n",
    "A Jupyter Notebook is a interavtive REPL (read–eval–print loop) tool. You can think of it as a \"advanced\" markdown editor with a built-in coding possibility. \n",
    "There are two terms that might be new if you have never used or seen a Jupyter Notebook before.\n",
    "\n",
    " - `Kernel` - A \"process\" that executes the code that are written in a notebook. You can look on it as a runtime. There are kernels for multiple languages. Python, R, Julia, C#, Java are some of them.\n",
    " - `Cell` - A notebook is split up in multiple cells. A cell is a container for ether text or code.\n",
    " \n",
    " <img src=\"img/Notebook-interface.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    " \n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cells\n",
    "\n",
    "Cells are the main components in a workbook. There are 3 cell types, but we will only cover two of them. The `Raw` cell type will note be used.\n",
    "\n",
    "The two cell types we will be using is:\n",
    "\n",
    " - `Code cell` - This cell contains the code that will be executed\n",
    " - `Markdown cell` - This cell contains markdown that will be rendered when executed.\n",
    "\n",
    "When you start a notebook there will always be one cell ready. This cell is a `code cell`. \n",
    "All new cells that are created will be a `code cell` by default. \n",
    "You have to change it to a `markdown cell` if you want to write markdown instead.\n",
    "\n",
    "![Notebook menu](img/Notebook-menu.png)\n",
    "\n",
    "To change it to a markdown cell you just have to press `m` when you have the cell selected. (Not when the cursor is inside the cell).\n",
    "To change it back to a code cell you do the same and press `y` instead. \n",
    "You can also do this by using the dropdown at the top of the notebook indicating the cell type. (As marked in orange in the image)\n",
    "\n",
    "To execute a cell you can press `Shift-Enter`. This executes a cell and creates a new code cell after that cell if there is no other cell to advance to.\n",
    "If you press `Ctrl-Enter` instead you execute a cell, but do not advance to the next cell or create a new cell.\n",
    "You can also execute or stop the execution of a cell by using the start and stop buttons highlighted in yellow and green in the image above.\n",
    "\n",
    "The results of a cell execution will be printed beneath the cell if the cell is a code cell. If it is a markdown cell then the resulting html of the markdown will be shown in the cell instead.\n",
    "\n",
    "To quickly edit a markdown cell after it has been rendered, you can just press `Enter`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Testing out Jupyter\n",
    "\n",
    "So now that you have recieved a crash course in the use of Jupyter Notebooks. Let us give it a try!\n",
    "\n",
    "First exercercise is to write a python snippet that prints hello world. The expected result is the words \"Hello world\" printed beneath the cell.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1 - Hello jupyter world!\n",
    "#\n",
    "# Write the code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope\n",
    "\n",
    "Great! You hav now run your first cell in a Jupyter Notebook! \n",
    "\n",
    "The code in each code cell belongs to the same scope.\n",
    "So this means that variables, functions or classes declared in one cell can be used in another cell. (So imports as well)\n",
    "Just remember to run the cell firsts. If a cell has not been run, the code it contains are not run and any variables, functions or classes are not declared.\n",
    "\n",
    "So let us test this out as well.\n",
    "\n",
    "**NOTE:** If a variable, function or class has been declared in a cell, and the cell has been executes. Than the variable will exist as long as the workbook are runnuing.\n",
    "Even if you clear out the cell and run the cell again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto complete\n",
    "\n",
    "Jupyter Notebooks has a limited support for autocomplete. If you press `TAB` then it will try to help you with autocompletion.\n",
    "\n",
    "If you need to look up the docstring on any Python code then you can move your cursor to the keyword, method or class and press `Shift-Tab` for the docstring if there exists one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Multiple cells\n",
    "\n",
    "Create string variable containing \"Hello world\" in one cell and print the content of that string in another cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Create your variable here\n",
    "#\n",
    "# Write the code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2 - Print your variable to the console here\n",
    "#\n",
    "# Write the code below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "\n",
    "NumPy is a Python package that are mainly used for scientific computing. It has a near C like performance. [[ref](https://www.ibm.com/developerworks/community/blogs/jfp/entry/A_Comparison_Of_C_Julia_Python_Numba_Cython_Scipy_and_BLAS_on_LU_Factorization?lang=en)] \n",
    "\n",
    "It is used to process arrays, multidimensional arrays and to perform mathematical operations on these arrays.\n",
    "\n",
    "NumPy’s array class is called `ndarray`. It is also known by the alias `array`. Note that `numpy.array` is not the same as the Standard Python Library class `array.array`, which we covered in the previous workshop and is not really recomended to use. (It was very limited in what it could do.)\n",
    "\n",
    "As well as multiple functions to process arrays, NumPy also has support for more data types than what Python has. This a long list that I will not paste here, but if you like to know more about the data types you can play around with then you can visit the documentation on [Data Types] (https://numpy.org/devdocs/user/basics.types.html)\n",
    "\n",
    "Many of the methods in the library and methods on the `ndarray` class has a optional `axis: int` parameter. This can be a confusing paramter to use. If you want to do the operartion on the **columns** then you can pass along `axis=0`. And if you want to do the operation on the **rows** then you can pass along `axis=1`. If no axis is supplied then `None` is the default value and the operation will be on all the values in the multidimensional array.\n",
    "\n",
    "So let us see some of what NumPy can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a array with range from 0 to 15. (Very much same as range(), just a ndarray instead of an sequence)\n",
    "array: np.ndarray = np.arange(15)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the array into a multidimensional array\n",
    "multidim: np.ndarray = array.reshape(3,5)\n",
    "print(multidim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also create a array by using the np.array() method\n",
    "array: np.ndarray = np.array([1,2,3,4])\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty arrays using np.zeroes()\n",
    "array: np.ndarray = np.zeros((3,4))\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arithmetic operators on arrays apply elementwise. A new array is created and filled with the result.\n",
    "a: np.ndarray = np.array([20,30,40,50])\n",
    "b: np.ndarray = np.array([0,1,2,3])\n",
    "print(a-b)\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The product operator * operates elementwise in NumPy arrays. \n",
    "#       The matrix product can be performed using the @ operator (in python >=3.5) \n",
    "#       or the dot function or method\n",
    "\n",
    "print(a @ b)\n",
    "print(a.dot(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten a multidimensional array back to a linear one\n",
    "flatarray: np.ndarray = array.ravel();\n",
    "print(flatarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array indexing is identical to Python's indexing scheme\n",
    "print(multidim[0])\n",
    "print(multidim[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also select elements in the array by conditions\n",
    "# Like this where we select all items that is \"NaN\" in the array\n",
    "a: np.ndarray = np.array([20,30,np.nan,50])\n",
    "print(a[np.isnan(a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python's concept of lists slicing is extended to NumPy.\n",
    "\n",
    "print(multidim[::-1]) # Reverse the rows\n",
    "print(multidim[::2])  # Every second row\n",
    "print(multidim[::,::2]) # Every second column\n",
    "print(multidim[0:2, 0:2]) # First two rows and the first two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndarray usefull attributes\n",
    "\n",
    "print(multidim.shape) # Returns a tuple consisting of array dimensions\n",
    "print(multidim.ndim) # Attribute returns the number of array dimensions\n",
    "print(multidim.itemsize) # Returns the length of each element of array in bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful methods\n",
    "\n",
    "# Average (Entire array)\n",
    "avg: float = np.average(np.array([[1,2,3,2,3],\n",
    "                                 [4,3,4,5,6]]))\n",
    "print(avg)\n",
    "\n",
    "# Average (By rows)\n",
    "avg = np.average(np.array([[1,2,3,2,3],\n",
    "                           [4,3,4,5,6]]), axis=1)\n",
    "print(avg)\n",
    "\n",
    "# Average (By columns)\n",
    "avg = np.average(np.array([[1,2,3,2,3],\n",
    "                           [4,3,4,5,6]]), axis=0)\n",
    "print(avg)\n",
    "\n",
    "# NOTE: The axis parameter determines if the average should be done over rows or columns. \n",
    "#       If no axis is set then entire array is used.\n",
    "#       This parameter named \"axis\" is present on many of the NumPy methods. Including sum described below.\n",
    "\n",
    "# Sum\n",
    "sum: float = np.sum(np.array([1,2,3,2,3,4,3,4,5,6]))\n",
    "print(sum)\n",
    "\n",
    "# Intersect\n",
    "a = np.array([1,2,3,2,3,4,3,4,5,6])\n",
    "b = np.array([7,2,10,2,7,4,9,4,9,8])\n",
    "print(np.intersect1d(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Wash data (NumPy)\n",
    "\n",
    "Given the multidimensional array below. Remove all rows that contains NaN.\n",
    "\n",
    "Expexted output: \n",
    "```\n",
    "[[ 1.  2.  3.  4.]\n",
    " [10.  3.  7.  6.]\n",
    " [ 6.  8. 15.  4.]]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3 - Wash the numpy data\n",
    "#\n",
    "# Write the code below\n",
    "\n",
    "import math\n",
    "array: np.ndarray = np.array([[1,2,3,4],\n",
    "                              [10,3,7,6],\n",
    "                              [3,math.nan,3,9],\n",
    "                              [6,8,15,4]])\n",
    "\n",
    "nanInRow: np.ndarray = np.array([~np.any(np.isnan(row)) for row in array])\n",
    "washedArray: np.ndarray = array[nanInRow]\n",
    "print(washedArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - More washing of data (NumPy)\n",
    "\n",
    "Given the same array as in exercise 3, instead of removing the \"bad\" row. Lets wash the data and insert the average of the resulting array from exercise 3 instead of `nan`.\n",
    "\n",
    "Expexted output: \n",
    "```\n",
    "[[ 1.,  2.,  3.,  4.],\n",
    "[10.,  3.,  7.,  6.],\n",
    "[ 3., 5.75,  3.,  9.],\n",
    "[ 6.,  8., 15.,  4.]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4 - More washing of data\n",
    "#\n",
    "# Write the code below\n",
    "\n",
    "array[np.isnan(array)] = np.average(washedArray)\n",
    "print(array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Pandas is widely used library that is used to _wrangle_ the data. The name stands for “Python Data Analysis Library”.\n",
    "\n",
    "Pandas can open and read files of the following types:\n",
    "\n",
    " - CSV\n",
    " - Json\n",
    " - HTML\n",
    " - MS Excel\n",
    " - OpenDocument\n",
    " - HDF5\n",
    " - Feather\n",
    " - Parquet\n",
    " - Msgpack\n",
    " - Strata\n",
    " - SAS\n",
    " - Python Pickle\n",
    " \n",
    "It can also connect to many different SQL databases. Some are:\n",
    "\n",
    " - SQLite\n",
    " - MySQL\n",
    " - PostgreSQL\n",
    " - MariaDB\n",
    " - SQL Server (using pyodbc library)\n",
    "\n",
    "Pandas reads data into a `DataFrame`. This is a Python object with rows and columns. For people with experience with `R` then this is very similar to the built-in datatype in R. \n",
    "The `DataFrame` can be viewed as a tabular data model. Using Pandas we can work with data in a very \"_SQL like way\", just better! I often just reads datasets from a database and wrangle the data using Pandas instead of writing a very intricate SQL query that would do the same.)\n",
    "\n",
    "Pandas has mainly two datatypes that we will use:\n",
    " - `Series`\n",
    " - `DataFrame`\n",
    "\n",
    "The `DataFrame` is built up of a set of `Series`. \n",
    "A column in a DataFrame is a Series. A row in a DataFrame is a value in multiple series that has the same index. (Most often a timestamp, ID or could just be a incremented counter.)\n",
    "You can name both columns and rows if you like. Columns wil often use the header available in the source files for names. But you can rename them easily enough later for easier reading.\n",
    "\n",
    "Using Pandas you can work on `DataFrame`s using the many built-in functions on the DataFrame class. We will go trough a subset of the functions below.\n",
    "\n",
    "\n",
    "### Creating a DataFrame\n",
    "\n",
    "You can open multiple types of files using Pandas. Here are some functions to read different data files. If you want to have a look at all the functions available for opening files then you can have a look at [IO Tools](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open file (Text, Csv, Excel, Json, etc)\n",
    "csvdata: pd.DataFrame = pd.read_csv(\"data/titanic_test.csv\")\n",
    "    \n",
    "# Open Json\n",
    "jsondata: pd.DataFrame = pd.read_json(\"data/iris.json\")\n",
    "\n",
    "# Open json from a url\n",
    "urldata: pd.DataFrame = pd.read_json(\"https://raw.githubusercontent.com/ETroll/python-pandas-workshop/master/data/data.json\")\n",
    "\n",
    "    \n",
    "# Using local data from memory\n",
    "data: dict = { \n",
    "     'letters' : [\"a\", \"b\", \"c\", \"d\", \"e\",\"f\", \"g\"],\n",
    "     'numbers' : [20,27, 35, 55, 18, 21, 35],\n",
    "     'data': [\"AA\", \"BB\", \"CC\", \"DD\", \"EE\", \"FF\", \"GG\"]\n",
    "}\n",
    "dictdata: pd.DataFrame = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting a DataFrame (or Series)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top 5 rows\n",
    "csvdata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the last 5 rows\n",
    "csvdata.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show statistics about the data\n",
    "csvdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 5 first from the \"Names\" column\n",
    "csvdata[\"Name\"].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics on DataFrame\n",
    "\n",
    "There are many methods to get basic statistics of the data inside a DataFrame. These methods uses the same `axis` parameter as in Numpy where:\n",
    "```\n",
    "Axis=0 => Rows (Default in most cases)\n",
    "Axis=1 => Columns\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.mean() Returns the mean of all columns\n",
    "csvdata.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvdata[\"Fare\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.corr() Returns the correlation between columns in a data frame\n",
    "csvdata.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.count() Returns the number of non-null values in each data frame column\n",
    "csvdata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.max() Returns the highest value in each column\n",
    "csvdata.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.min() Returns the lowest value in each column\n",
    "csvdata.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.median() Returns the median of each column\n",
    "csvdata.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.std() Returns the standard deviation of each column\n",
    "csvdata.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending columns (Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.concat([df1, df2]) — add the columns in df1 to the end of df2 (rows should be identical)\n",
    "firstdata: pd.DataFrame = pd.DataFrame({\"first\": ['A', 'B', 'C']})\n",
    "seconddata: pd.DataFrame = pd.DataFrame({\"second\": ['D', 'E', 'F']})\n",
    "pd.concat([firstdata, seconddata], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just add a column by accessing the new column in the dataframe and set the data.\n",
    "csvdata[\"NewColumn\"] = \"Newdata\"\n",
    "csvdata.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign new column to dataframe using lambda\n",
    "csvdata.assign(Age_times_fare=lambda x: x.Fare * x.Age).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use apply() to create a new column using the data from a single column\n",
    "csvdata[\"DoubleAge\"] = csvdata[\"Age\"].apply(lambda x: x*2 if pd.notnull(x) else 0)\n",
    "csvdata.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.concat([df1, df2]) — add the rows in df1 to the end of df2\n",
    "firstdata: pd.DataFrame = pd.DataFrame(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "seconddata: pd.DataFrame = pd.DataFrame(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "pd.concat([firstdata, seconddata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.append(df2) — add the df2 at the end of df\n",
    "firstdata: pd.DataFrame = pd.DataFrame(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "seconddata: pd.DataFrame = pd.DataFrame(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "firstdata.append(seconddata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleanup\n",
    "\n",
    "Data cleanup is a very important task in Pandas and there are several methods to help us here.\n",
    "\n",
    "\n",
    "    df.dropna() - Drop rows containing na\n",
    "    df.fillna() - \n",
    "    df.rename(columns={'old_name': 'new_ name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.isnull() - Returns a boolean array/dataframe (an array/dataframe of true for missing values and false for non-missing values).\n",
    "csvdata.isnull().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.notnull() - Returns the opposite of isnull()\n",
    "csvdata.notnull().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dropna() - Drops rows containing NA\n",
    "csvdata.dropna().tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.fillna() - Fills datacells containing Na with the given value\n",
    "csvdata.fillna(0).tail(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills every Na with the mean value\n",
    "csvdata.fillna(csvdata.mean()).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.rename(columns={'old_name': 'new_ name'}) - Rename a column\n",
    "csvdata.rename(columns={'SibSp': 'SiblingSpouse'}).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "You can use different conditions to filter columns. For example, `df[df[\"Age\"] > 30]`. You can also use & (and) or | (or) to add different conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvdata[\"PassengerId\"][csvdata[\"Age\"] > 50].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvdata[csvdata[\"Age\"] > 50].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvdata[(csvdata[\"Age\"] > 50) & (csvdata[\"SibSp\"] == 2)].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering using the query method \n",
    "csvdata.query('Age > 50 and SibSp == 2') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by column values - Descending\n",
    "csvdata.sort_values(\"Age\").head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by column values - Ascending\n",
    "csvdata.sort_values(\"Age\", ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by multiple columns\n",
    "csvdata.sort_values(by=['Age', 'Fare'], ascending=False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group dataframe by column - Get all female passengers\n",
    "grouped = csvdata.groupby(\"Sex\")\n",
    "grouped.get_group('female').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = csvdata.groupby([\"Sex\", \"Embarked\"])\n",
    "grouped.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining\n",
    "\n",
    "    \n",
    "    df1.join(df2,on=col1,how='inner') — SQL-style join the columns in df1 with the columns on df2 where the rows for colhave identical values. how can be equal to one of: 'left', 'right', 'outer', 'inner'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join using a common index\n",
    "firstdata: pd.DataFrame = pd.DataFrame({\"first\": ['A', 'B', 'C'], \"key\": [1,1,3]})\n",
    "seconddata: pd.DataFrame = pd.DataFrame({\"second\": ['D', 'E', 'F'], \"key\": [1,1,2]})\n",
    "firstdata.set_index(\"key\").join(seconddata.set_index(\"key\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join using a common collumn. Note that the right and left suffixes are required\n",
    "firstdata: pd.DataFrame = pd.DataFrame({\"first\": ['A', 'B', 'C'], \"key\": [1,1,3]})\n",
    "seconddata: pd.DataFrame = pd.DataFrame({\"second\": ['D', 'E', 'F'], \"key\": [1,1,2]})\n",
    "firstdata.join(seconddata, on=\"key\", how=\"inner\", lsuffix='_left', rsuffix='_right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Import data \n",
    "\n",
    "For all the Pandas exercises we will be using the infamous Titanic dataset. This dataset is split in two, training and test set. Since we will not be training any machine learning model of sorts here we can just combine the two files into one dataframe.\n",
    "\n",
    "So for the first of the Pandas exercises: Open the two csv files, `titanic_test.csv` and `titanic_train.csv` and combine them into one Pandas dataframe and show the last 10 rows.\n",
    "\n",
    "Expected output for `tail(10)`: \n",
    "\n",
    "|  |PassengerId |Pclass |Name |Sex |Age |SibSp |Parch |Ticket |Fare |Cabin |Embarked |Survived \n",
    "|---: |---: |---: |---: |---: |---: |---: |---: |---: |---: |---: |---: | ---:\n",
    "|881 |882 |3 |Markun, Mr. Johann |male |33.0 |0 |0 |349257 |7.8958 |NaN |S |0.0 | \n",
    "|882 |883 |3 |Dahlberg, Miss. Gerda Ulrika |female |22.0 |0 |0 |7552 |10.5167 |NaN |S |0.0 | \n",
    "|883 |884 |2 |Banfield, Mr. Frederick James |male |28.0 |0 |0 |C.A./SOTON 34068 |10.5000 |NaN |S |0.0 | \n",
    "|884 |885 |3 |Sutehall, Mr. Henry Jr |male |25.0 |0 |0 |SOTON/OQ 392076 |7.0500 |NaN |S |0.0 |\n",
    "|885 |886 |3 |Rice, Mrs. William (Margaret Norton) |female |39.0 |0 |5 |382652 |29.1250 |NaN |Q |0.0 |\n",
    "|886 |887 |2 |Montvila, Rev. Juozas |male |27.0 |0 |0 |211536 |13.0000 |NaN |S |0.0 |\n",
    "|887 |888 |1 |Graham, Miss. Margaret Edith |female |19.0 |0 |0 |112053 |30.0000 |B42 |S |1.0 |\n",
    "|888 |889 |3 |Johnston, Miss. Catherine Helen \"Carrie\" |female |NaN |1 |2 |W./C. 6607 |23.4500 |NaN |S |0.0 |\n",
    "|889 |890 |1 |Behr, Mr. Karl Howell |male |26.0 |0 |0 |111369 |30.0000 |C148 |C| 1.0 |\n",
    "|890 |891 |3 |Dooley, Mr. Patrick| male| 32.0 |0 |0 |370376 |7.7500 |NaN |Q |0.0|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5 - Import data into a Pandas dataframe\n",
    "#\n",
    "# Write the code below\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "testdata: pd.DataFrame = pd.read_csv(\"data/titanic_test.csv\")\n",
    "trainingdata: pd.DataFrame = pd.read_csv(\"data/titanic_train.csv\")\n",
    "\n",
    "data: pd.DataFrame = testdata.append(trainingdata, sort=False)\n",
    "data.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6 - Wash the data\n",
    "\n",
    "As you might have noticed in exercise 5, the data has some blanks that needs to be washed. There are also some columns that we might not need further on. So let us remove those as well.\n",
    "\n",
    "So for this exercise: \n",
    " - Remove the columns `PassengerId` and `Ticket`\n",
    " - Fill the missing `Cabin` data with the value: `X`\n",
    " - Fill the missing `Fare` data with an average for the `Fare` column\n",
    " - Fill the missing `Age` data with an averate for the `Age` column\n",
    " - Rename the `SibSp` column to `NumSiblingSpouses`\n",
    " - Rename the `Parch` column to `NumParentsChildren`\n",
    "\n",
    "Expected output for `tail()`:\n",
    "\n",
    "\n",
    "| |Pclass |Name |Sex |Age |NumSiblingSpouses |NumParentsChildren |Fare |Cabin |Embarked |Survived\n",
    "| --: |--: |--: |--: |--: |--: |--: |--: |--: |--: |--:\n",
    "886 |2 |Montvila, Rev. Juozas |male |27.000000 |0 |0 |13.00 |X |S |0.0\n",
    "887 |1 |Graham, Miss. Margaret Edith |female |19.000000 |0 |0 |30.00 |B42 |S |1.0\n",
    "888 |3 |Johnston, Miss. Catherine Helen \"Carrie\" |female |29.881138 |1 |2 |23.45 |X |S |0.0\n",
    "889 |1 |Behr, Mr. Karl Howell |male |26.000000 |0 |0 |30.00 |C148 |C |1.0\n",
    "890 |3 |Dooley, Mr. Patrick |male |32.000000 |0 |0 |7.75 |X |Q |0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6 - Wash the Titanic dataset\n",
    "#\n",
    "# Write the code below\n",
    "\n",
    "data.drop(\"PassengerId\", axis=1, inplace=True)\n",
    "data.drop(\"Ticket\", axis=1, inplace=True)\n",
    "\n",
    "data['Cabin'].fillna(\"X\", inplace=True)\n",
    "data['Fare'].fillna(data[\"Fare\"].mean(), inplace=True)\n",
    "data['Age'].fillna(data[\"Age\"].mean(), inplace=True)\n",
    "\n",
    "data.rename(columns={\"SibSp\":\"NumSiblingSpouses\",\n",
    "                      \"Parch\":\"NumParentsChildren\"}, \n",
    "                 inplace=True)\n",
    "\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7 - Filtering\n",
    "\n",
    "From the combined dataframe with the changes made in Exercise 6, create a new `DataFrame` that contains all the passengers that travelled alone. (Had no siblings, spouses, parents or children travelling with them)\n",
    "\n",
    "Expexted output for `tail()`:\n",
    "\n",
    "\n",
    "| |Pclass |Name |Sex |Age |NumSiblingSpouses |NumParentsChildren |Fare |Cabin |Embarked |Survived\n",
    "| --: |--: |--: |--: |--: |--: |--: |--: |--: |--: |--:\n",
    "884 |3 |Sutehall, Mr. Henry Jr |male |25.0 |0 |0 |7.05 |X |S |0.0\n",
    "886 |2 |Montvila, Rev. Juozas |male |27.0 |0 |0 |13.00 |X |S |0.0\n",
    "887 |1 |Graham, Miss. Margaret Edith |female |19.0 |0 |0 |30.00 |B42 |S |1.0\n",
    "889 |1 |Behr, Mr. Karl Howell |male |26.0 |0 |0 |30.00 |C148 |C |1.0\n",
    "890 |3 |Dooley, Mr. Patrick |male |32.0 |0 |0 |7.75 |X |Q |0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7 - Filtering\n",
    "#\n",
    "# Write the code below for creating the new dataframe\n",
    "alone: pd.DataFrame = data[(data[\"NumSiblingSpouses\"] == 0) & (data[\"NumParentsChildren\"] == 0)]\n",
    "# alone: pd.DataFrame = data.query('NumSiblingSpouses == 0 and NumParentsChildren == 0')\n",
    "alone.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8 - Create a DataFrame using results\n",
    "\n",
    "Create a new `DataFrame` with named two named columns and four named rows (indexes) that has a column names `Alone` for the passengers travelling alone, and another column `Family` for the passengers travelling with family.\n",
    "The rows are named `MaxAge`, `MinAge`, `MeanAge` and `Count` and contains the max, min and mean age as well as count for passengers that travels alone or with family.\n",
    "\n",
    "Hint: _Google how to create a DataFrame with a index for the named \"rows\". (Or you could just create 3 columns and use the first column for the name)_\n",
    "\n",
    "Expected output:\n",
    "\n",
    "\n",
    "| | Alone| Family\n",
    "| --: | --: | --:\n",
    "MaxAge| 80.000000| 76.000000\n",
    "MinAge| 5.000000| 0.170000\n",
    "MeanAge| 31.099022| 28.027325\n",
    "Count| 790.000000| 519.000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8 - Create a DataFrame using results\n",
    "#\n",
    "# Write the code below\n",
    "\n",
    "# family : pd.DataFrame = data.query('NumSiblingSpouses > 0 or NumParentsChildren > 0')\n",
    "family: pd.DataFrame = data[(data[\"NumSiblingSpouses\"] > 0) | (data[\"NumParentsChildren\"] > 0)]\n",
    "\n",
    "datadict: dict = {\n",
    "    \"Alone\": [alone[\"Age\"].max(), alone[\"Age\"].min(), alone[\"Age\"].mean(), alone[\"Age\"].count()],\n",
    "    \"Family\": [family[\"Age\"].max(), family[\"Age\"].min(), family[\"Age\"].mean(), family[\"Age\"].count()]\n",
    "}\n",
    "    \n",
    "newframe: pd.DataFrame = pd.DataFrame(datadict, index =[\"MaxAge\", \"MinAge\", \"MeanAge\", \"Count\"]) \n",
    "newframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 - Create new columns (features)\n",
    "\n",
    "Creating new features in a dataset is very important in machine learning. And using Pandas this can be very easy to do.\n",
    "\n",
    "In the titanic dataset the first letter in the Cabin number is the deck that the cabin is located at.\n",
    "\n",
    "Create a new column names `Deck` that contains this deck. If the cabin number is `X` (we replaced missing cabin numbers with `X` earlier) then use the letter `M` for \"missing\".\n",
    "\n",
    "Hint: _Have a look at lambdas from last workshop_\n",
    "\n",
    "Expected output for `tail(5)`:\n",
    "\n",
    "| |Pclass |Name |Sex |Age |NumSiblingSpouses |NumParentsChildren |Fare |Cabin |Embarked |Survived | Deck\n",
    "| --: |--: |--: |--: |--: |--: |--: |--: |--: |--: |--: |--:\n",
    "886 |2 |Montvila, Rev. Juozas |male |27.000000 |0 |0 |13.00 |X |S |0.0 |M\n",
    "887 |1 |Graham, Miss. Margaret Edith |female |19.000000 |0 |0 |30.00 |B42 |S |1.0 |B\n",
    "888 |3 |Johnston, Miss. Catherine Helen \"Carrie\" |female |29.881138 |1 |2 |23.45 |X |S |0.0 |M\n",
    "889 |1 |Behr, Mr. Karl Howell |male |26.000000 |0 |0 |30.00 |C148 |C |1.0 |C\n",
    "890 |3 |Dooley, Mr. Patrick |male |32.000000 |0 |0 |7.75 |X |Q |0.0 |M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8 - Create new features\n",
    "#\n",
    "# Write the code below\n",
    "\n",
    "data[\"Deck\"] = data[\"Cabin\"].apply(lambda s: s[0] if s[0] is not \"X\" else \"M\")\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matplotlib\n",
    "\n",
    "Matplotlib needs a little mention at the end here. This workhop will not concentrate on teaching the basic usage of matplotlib. It is a library that is specially designed for drawing graphs, charts etc.\n",
    "Matplotlib is integrated into Pandas with the methods `plot()` and property `plot`. This mak\n",
    "\n",
    "Below is a few very simple examples how matplotlib can be used with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not really make sense to do, but why not?\n",
    "csvdata.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render a bar plot of the number of siblings and spouses.\n",
    "csvdata[\"SibSp\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram over the age distribution of the passengers\n",
    "csvdata[\"Age\"].plot.hist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
